# ======== data/data_utils.py ========

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

# åŸºäºå®é™…æ•°æ®ä¸­çš„å¤©æ°”ç‰¹å¾ï¼Œç®€åŒ–ä¸ºä¸¤ç§ç±»åˆ«
# å¤ªé˜³è¾å°„ç‰¹å¾ - æœ€é‡è¦çš„ç‰¹å¾
IRRADIANCE_FEATURES = [
    'global_tilted_irradiance',    # å…¨çƒå€¾æ–œè¾å°„ (æœ€é‡è¦çš„è¾å°„ç‰¹å¾)
]

# å…¨éƒ¨å¤©æ°”ç‰¹å¾ - åŒ…å«æ‰€æœ‰å¤©æ°”å˜é‡
ALL_WEATHER_FEATURES = [
    'global_tilted_irradiance',    # å…¨çƒå€¾æ–œè¾å°„
    'vapour_pressure_deficit',     # æ°´æ±½å‹å·®
    'relative_humidity_2m',        # ç›¸å¯¹æ¹¿åº¦
    'temperature_2m',              # æ¸©åº¦
    'wind_gusts_10m',             # 10ç±³é˜µé£
    'cloud_cover_low',            # ä½äº‘è¦†ç›–
    'wind_speed_100m',            # 100ç±³é£é€Ÿ
    'snow_depth',                 # é›ªæ·±
    'dew_point_2m',               # éœ²ç‚¹æ¸©åº¦
    'surface_pressure',           # è¡¨é¢æ°”å‹
    'precipitation',              # é™æ°´
]

# æ•æ„Ÿæ€§åˆ†æå¤©æ°”ç‰¹å¾å®šä¹‰
SOLAR_IRRADIANCE_FEATURES = ['global_tilted_irradiance']
HIGH_WEATHER_FEATURES = ['global_tilted_irradiance', 'vapour_pressure_deficit', 'relative_humidity_2m']
MEDIUM_WEATHER_FEATURES = HIGH_WEATHER_FEATURES + ['temperature_2m', 'wind_gusts_10m', 'cloud_cover_low', 'wind_speed_100m']
LOW_WEATHER_FEATURES = MEDIUM_WEATHER_FEATURES + ['snow_depth', 'dew_point_2m', 'surface_pressure', 'precipitation']

# æ ¹æ®å¤©æ°”ç‰¹å¾ç±»åˆ«é€‰æ‹©ç‰¹å¾
def get_weather_features_by_category(weather_category):
    """
    æ ¹æ®å¤©æ°”ç‰¹å¾ç±»åˆ«è¿”å›å¤©æ°”ç‰¹å¾
    
    Args:
        weather_category: 'none', 'all_weather', 'solar_irradiance_only', 'high_weather', 'medium_weather', 'low_weather', 'ablation_11_features'
    
    Returns:
        list: é€‰ä¸­çš„å¤©æ°”ç‰¹å¾åˆ—è¡¨
    """
    if weather_category == 'none':
        return []  # ä¸è¿”å›ä»»ä½•å¤©æ°”ç‰¹å¾
    elif weather_category == 'all_weather':
        return ALL_WEATHER_FEATURES
    elif weather_category == 'solar_irradiance_only':
        return SOLAR_IRRADIANCE_FEATURES
    elif weather_category == 'high_weather':
        return HIGH_WEATHER_FEATURES
    elif weather_category == 'medium_weather':
        return MEDIUM_WEATHER_FEATURES
    elif weather_category == 'low_weather':
        return LOW_WEATHER_FEATURES
    elif weather_category == 'ablation_11_features':
        return ALL_WEATHER_FEATURES  # ä½¿ç”¨å…¨éƒ¨å¤©æ°”ç‰¹å¾
    else:
        raise ValueError(f"Invalid weather_category: {weather_category}")

# ä¿æŒå‘åå…¼å®¹æ€§
BASE_HIST_FEATURES = IRRADIANCE_FEATURES
BASE_FCST_FEATURES = IRRADIANCE_FEATURES

# æ—¶é—´ç¼–ç ç‰¹å¾
TIME_FEATURES = ['month_cos', 'month_sin', 'hour_cos', 'hour_sin']

TARGET_COL = 'Capacity Factor'

# ç»Ÿè®¡ç‰¹å¾å‡½æ•°å·²ç§»é™¤

def load_raw_data(path: str) -> pd.DataFrame:
    df = pd.read_csv(path)
    df['Datetime'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']])
    return df

# def preprocess_features(df: pd.DataFrame, config: dict):
#     df_clean = df.dropna(subset=[TARGET_COL]).copy()

#     hist_feats = []
#     fcst_feats = []

#     if config.get('use_hist_weather', True):
#         hist_feats += BASE_HIST_FEATURES

#     if config.get('use_time', False):
#         for col in ('Month_cos', 'Hour_sin', 'Hour_cos'):
#             if col not in hist_feats:
#                 hist_feats.append(col)

#     if config.get('use_stats', False):
#         hist_feats += BASE_STAT_FEATURES

#     if config.get('use_forecast', False):
#         fcst_feats += BASE_FCST_FEATURES

#     # Drop rows with missing values in all relevant features
#     na_check_feats = hist_feats + fcst_feats + [TARGET_COL]
#     df_clean = df_clean.dropna(subset=na_check_feats).reset_index(drop=True)

#     if hist_feats:
#         scaler_hist = MinMaxScaler()
#         df_clean[hist_feats] = scaler_hist.fit_transform(df_clean[hist_feats])
#     else:
#         scaler_hist = None

#     scaler_target = MinMaxScaler()
#     df_clean[[TARGET_COL]] = scaler_target.fit_transform(df_clean[[TARGET_COL]])

#     if fcst_feats:
#         scaler_fcst = MinMaxScaler()
#         df_clean[fcst_feats] = scaler_fcst.fit_transform(df_clean[fcst_feats])
#     else:
#         scaler_fcst = None

#     df_clean = df_clean.sort_values('Datetime').reset_index(drop=True)

#     return df_clean, hist_feats, fcst_feats, scaler_hist, scaler_fcst, scaler_target
def preprocess_features(df: pd.DataFrame, config: dict):
    df_clean = df.dropna(subset=[TARGET_COL]).copy()

    # æ—¥æœŸè¿‡æ»¤ï¼šåªä½¿ç”¨2022-01-01ä¹‹åçš„æ•°æ®
    start_date = config.get('start_date', '2022-01-01')
    end_date = config.get('end_date', '2024-09-28')
    
    if start_date:
        start_dt = pd.to_datetime(start_date)
        df_clean = df_clean[df_clean['Datetime'] >= start_dt].copy()
        print(f"ğŸ“Š è¿‡æ»¤åæ•°æ®ï¼ˆä»{start_date}å¼€å§‹ï¼‰: {len(df_clean)}è¡Œ")
    
    if end_date:
        end_dt = pd.to_datetime(end_date)
        df_clean = df_clean[df_clean['Datetime'] <= end_dt].copy()
        print(f"ğŸ“Š è¿‡æ»¤åæ•°æ®ï¼ˆåˆ°{end_date}ç»“æŸï¼‰: {len(df_clean)}è¡Œ")

    # æ·»åŠ æ—¶é—´ç¼–ç ç‰¹å¾ï¼ˆæ ¹æ®å¼€å…³å†³å®šï¼‰
    use_time_encoding = config.get('use_time_encoding', True)
    if use_time_encoding:
        df_clean['month_cos'] = np.cos(2 * np.pi * df_clean['Month'] / 12)
        df_clean['month_sin'] = np.sin(2 * np.pi * df_clean['Month'] / 12)
        df_clean['hour_cos'] = np.cos(2 * np.pi * df_clean['Hour'] / 24)
        df_clean['hour_sin'] = np.sin(2 * np.pi * df_clean['Hour'] / 24)

    # æ„å»ºç‰¹å¾åˆ—è¡¨
    hist_feats = []
    fcst_feats = []

    # è·å–å¤©æ°”ç‰¹å¾ç±»åˆ«
    weather_category = config.get('weather_category', 'none')

    # PVç‰¹å¾ï¼ˆå†å²å‘ç”µé‡ï¼‰
    if config.get('use_pv', False):
        # åˆ›å»ºå†å²Capacity Factorç‰¹å¾ï¼ˆè¿‡å»24/72å°æ—¶çš„å‘ç”µé‡ï¼‰
        df_clean['Capacity_Factor_hist'] = df_clean[TARGET_COL]
        hist_feats.append('Capacity_Factor_hist')

    # å†å²å¤©æ°”ç‰¹å¾ï¼ˆHWï¼‰- ä¸å¸¦_predåç¼€
    if config.get('use_hist_weather', False):
        hist_feats += get_weather_features_by_category(weather_category)

    # æ—¶é—´ç¼–ç ç‰¹å¾ï¼ˆæ ¹æ®å¼€å…³å†³å®šï¼‰
    if use_time_encoding:
        hist_feats += TIME_FEATURES

    # é¢„æµ‹ç‰¹å¾ï¼ˆNWPï¼‰
    if config.get('use_forecast', False):
        if config.get('use_ideal_nwp', False):
            # ç†æƒ³NWPï¼šä½¿ç”¨ç›®æ ‡æ—¥çš„HWç‰¹å¾ï¼ˆä¸å¸¦_predåç¼€ï¼‰
            base_weather_features = get_weather_features_by_category(weather_category)
            fcst_feats += base_weather_features
        else:
            # æ™®é€šNWPï¼šä½¿ç”¨å¸¦_predåç¼€çš„é¢„æµ‹ç‰¹å¾
            base_weather_features = get_weather_features_by_category(weather_category)
            forecast_features = [f + '_pred' for f in base_weather_features]
            fcst_feats += forecast_features

    # ç¡®ä¿æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨
    available_hist_feats = [f for f in hist_feats if f in df_clean.columns]
    available_fcst_feats = [f for f in fcst_feats if f in df_clean.columns]

    # åˆ é™¤ç¼ºå¤±å€¼
    na_check_feats = available_hist_feats + available_fcst_feats + [TARGET_COL]
    df_clean = df_clean.dropna(subset=na_check_feats).reset_index(drop=True)

    # æ ‡å‡†åŒ–ç‰¹å¾
    scaler_hist = MinMaxScaler()
    if available_hist_feats:
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜å¼‚æ€§
        for feat in available_hist_feats:
            if df_clean[feat].std() == 0:
                print(f"âš ï¸ ç‰¹å¾ {feat} æ ‡å‡†å·®ä¸º0ï¼Œæ·»åŠ å¾®å°å™ªå£°é¿å…é™¤é›¶é”™è¯¯")
                df_clean[feat] += np.random.normal(0, 1e-8, len(df_clean))
        df_clean[available_hist_feats] = scaler_hist.fit_transform(df_clean[available_hist_feats])

    scaler_fcst = MinMaxScaler()
    if available_fcst_feats:
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦æœ‰è¶³å¤Ÿçš„å˜å¼‚æ€§
        for feat in available_fcst_feats:
            if df_clean[feat].std() == 0:
                print(f"âš ï¸ ç‰¹å¾ {feat} æ ‡å‡†å·®ä¸º0ï¼Œæ·»åŠ å¾®å°å™ªå£°é¿å…é™¤é›¶é”™è¯¯")
                df_clean[feat] += np.random.normal(0, 1e-8, len(df_clean))
        df_clean[available_fcst_feats] = scaler_fcst.fit_transform(df_clean[available_fcst_feats])

    # Capacity Factorä¸éœ€è¦æ ‡å‡†åŒ–ï¼ˆèŒƒå›´0-100ï¼‰
    scaler_target = None

    df_clean = df_clean.sort_values('Datetime').reset_index(drop=True)

    return df_clean, available_hist_feats, available_fcst_feats, scaler_hist, scaler_fcst, scaler_target

def create_sliding_windows(df, past_hours, future_hours, hist_feats, fcst_feats, no_hist_power=False):
    """
    åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬ï¼Œå…è®¸æ—¶é—´ä¸è¿ç»­
    æ¯ä¸ªæ ·æœ¬åŒ…å«ï¼šå‰nå¤©å†å²æ•°æ® + é¢„æµ‹å½“å¤©çš„é¢„æµ‹æ•°æ®
    
    Args:
        no_hist_power: å¦‚æœä¸ºTrueï¼Œä¸ä½¿ç”¨å†å²å‘ç”µé‡æ•°æ®ï¼Œåªä½¿ç”¨é¢„æµ‹å¤©æ°”
    """
    X_hist, y, hours, dates = [], [], [], []
    X_fcst = [] if fcst_feats else None  # åªæœ‰åœ¨éœ€è¦é¢„æµ‹ç‰¹å¾æ—¶æ‰åˆå§‹åŒ–
    n = len(df)
    
    # æŒ‰å¤©åˆ†ç»„ï¼Œæ¯å¤©24å°æ—¶
    df['Date'] = df['Datetime'].dt.date
    daily_groups = df.groupby('Date')
    daily_dates = list(daily_groups.groups.keys())
    
    # ç¡®ä¿æœ‰è¶³å¤Ÿçš„å†å²å¤©æ•°
    if no_hist_power:
        min_days = 0  # ä»…é¢„æµ‹å¤©æ°”æ¨¡å¼ä¸éœ€è¦å†å²æ•°æ®
    else:
        min_days = past_hours // 24 + 1  # è‡³å°‘éœ€è¦è¿™ä¹ˆå¤šå¤©
    
    if len(daily_dates) < min_days + 1:  # +1 for prediction day
        raise ValueError(f"æ•°æ®ä¸è¶³ï¼šéœ€è¦è‡³å°‘{min_days + 1}å¤©çš„æ•°æ®")
    
    # ä¸ºæ¯ä¸ªé¢„æµ‹æ—¥åˆ›å»ºæ ·æœ¬
    for pred_date_idx in range(min_days, len(daily_dates)):
        pred_date = daily_dates[pred_date_idx]
        pred_day_data = daily_groups.get_group(pred_date)
        
        if no_hist_power:
            # æ— å†å²å‘ç”µé‡æ¨¡å¼ï¼šåªä½¿ç”¨é¢„æµ‹å¤©æ°”æ•°æ®
            fut_win = pred_day_data.head(future_hours)
            
            if len(fut_win) < future_hours:
                continue
            
            # æ„å»ºæ ·æœ¬ï¼ˆåªæœ‰é¢„æµ‹ç‰¹å¾ï¼‰
            if fcst_feats:
                X_fcst.append(fut_win[fcst_feats].values)
            
            y.append(fut_win[TARGET_COL].values)
            hours.append(fut_win['Hour'].values)
            dates.append(fut_win['Datetime'].iloc[-1])
            
            # å¯¹äºæ— å†å²å‘ç”µé‡æ¨¡å¼ï¼ŒX_histä¸ºç©º
            X_hist.append(np.array([]).reshape(0, len(hist_feats)) if hist_feats else np.array([]).reshape(0, 0))
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šä½¿ç”¨å†å²æ•°æ®
            # æ”¶é›†å†å²æ•°æ®ï¼ˆå‰nå¤©ï¼‰
            hist_data = []
            for hist_date_idx in range(max(0, pred_date_idx - min_days), pred_date_idx):
                hist_date = daily_dates[hist_date_idx]
                hist_day_data = daily_groups.get_group(hist_date)
                hist_data.append(hist_day_data)
            
            if len(hist_data) == 0:
                continue
                
            # åˆå¹¶å†å²æ•°æ®
            hist_combined = pd.concat(hist_data, ignore_index=True)
            
            # å¦‚æœå†å²æ•°æ®ä¸è¶³past_hoursï¼Œè·³è¿‡
            if len(hist_combined) < past_hours:
                continue
                
            # å–æœ€åpast_hourså°æ—¶çš„å†å²æ•°æ®
            hist_win = hist_combined.tail(past_hours)
            
            # é¢„æµ‹æ•°æ®ï¼ˆé¢„æµ‹å½“å¤©çš„æ•°æ®ï¼‰
            fut_win = pred_day_data.head(future_hours)
            
            if len(fut_win) < future_hours:
                continue
            
            # æ„å»ºæ ·æœ¬
            X_hist.append(hist_win[hist_feats].values)
            
            if fcst_feats:
                # é¢„æµ‹å¤©æ°”ï¼šä½¿ç”¨é¢„æµ‹å½“å¤©çš„å¤©æ°”æ•°æ®
                X_fcst.append(fut_win[fcst_feats].values)
            
            y.append(fut_win[TARGET_COL].values)
            hours.append(fut_win['Hour'].values)
            dates.append(fut_win['Datetime'].iloc[-1])
    
    if len(X_hist) == 0:
        raise ValueError("æ— æ³•åˆ›å»ºä»»ä½•æœ‰æ•ˆæ ·æœ¬")
    
    X_hist = np.stack(X_hist)
    y = np.stack(y)
    hours = np.stack(hours)
    X_fcst = np.stack(X_fcst) if fcst_feats else None

    return X_hist, X_fcst, y, hours, dates

def split_data(X_hist, X_fcst, y, hours, dates, train_ratio=0.8, val_ratio=0.1, shuffle=True, random_state=42):
    """
    åˆ†å‰²æ•°æ®ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    ç”±äºæ ·æœ¬å·²ç»æ˜¯éè¿ç»­çš„æ—¶é—´çª—å£ï¼Œå¯ä»¥å®‰å…¨åœ°shuffleå’ŒæŒ‰æ¯”ä¾‹åˆ†å‰²
    æ¯ä¸ªæ ·æœ¬éƒ½æ˜¯ç‹¬ç«‹çš„é¢„æµ‹æ—¥ï¼Œä¸å­˜åœ¨æ•°æ®æ³„æ¼é—®é¢˜
    """
    N = X_hist.shape[0]
    
    # åˆ›å»ºéšæœºç´¢å¼•
    if shuffle:
        np.random.seed(random_state)
        indices = np.random.permutation(N)
    else:
        indices = np.arange(N)
    
    # è®¡ç®—åˆ†å‰²ç‚¹
    i_tr = int(N * train_ratio)
    i_val = int(N * (train_ratio + val_ratio))
    
    # åˆ†å‰²ç´¢å¼•
    train_idx = indices[:i_tr]
    val_idx = indices[i_tr:i_val]
    test_idx = indices[i_val:]
    
    # å®šä¹‰åˆ‡ç‰‡å‡½æ•°
    def slice_array(arr, indices):
        if isinstance(arr, np.ndarray):
            return arr[indices]
        else:
            # å¤„ç†åˆ—è¡¨ç±»å‹
            return [arr[i] for i in indices]

    # åˆ†å‰²æ‰€æœ‰æ•°ç»„
    Xh_tr, Xh_va, Xh_te = slice_array(X_hist, train_idx), slice_array(X_hist, val_idx), slice_array(X_hist, test_idx)
    y_tr, y_va, y_te = slice_array(y, train_idx), slice_array(y, val_idx), slice_array(y, test_idx)
    hrs_tr, hrs_va, hrs_te = slice_array(hours, train_idx), slice_array(hours, val_idx), slice_array(hours, test_idx)
    
    # å¤„ç†æ—¥æœŸåˆ—è¡¨
    dates_tr = [dates[i] for i in train_idx]
    dates_va = [dates[i] for i in val_idx]
    dates_te = [dates[i] for i in test_idx]

    # å¤„ç†é¢„æµ‹ç‰¹å¾
    if X_fcst is not None:
        Xf_tr, Xf_va, Xf_te = slice_array(X_fcst, train_idx), slice_array(X_fcst, val_idx), slice_array(X_fcst, test_idx)
    else:
        Xf_tr = Xf_va = Xf_te = None

    return (
        Xh_tr, Xf_tr, y_tr, hrs_tr, dates_tr,
        Xh_va, Xf_va, y_va, hrs_va, dates_va,
        Xh_te, Xf_te, y_te, hrs_te, dates_te
    )

