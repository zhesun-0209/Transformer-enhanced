# å®éªŒè¾“å‡ºè¯´æ˜

## è¿è¡Œ `python run_experiments.py` çš„è¾“å‡ºå’Œç»“æœ

### ğŸ–¥ï¸ æ§åˆ¶å°è¾“å‡º

è¿è¡Œå®éªŒæ—¶ï¼Œæ‚¨ä¼šåœ¨æ§åˆ¶å°çœ‹åˆ°ä»¥ä¸‹è¾“å‡ºï¼š

#### 1. å®éªŒå¼€å§‹ä¿¡æ¯
```
ğŸ”¬ Transformeræ¶æ„å¯¹æ¯”å®éªŒ
==================================================

============================================================
å®éªŒ 1/4: åŸå§‹Transformer (Last Timestep)
============================================================

ğŸš€ å¼€å§‹å®éªŒ: åŸå§‹Transformer (Last Timestep)
ğŸ“ é…ç½®æ–‡ä»¶: Transformer_low_PV_plus_NWP_24h_noTE.yaml
â° å¼€å§‹æ—¶é—´: 2024-09-23 23:30:15
```

#### 2. è®­ç»ƒè¿‡ç¨‹è¾“å‡º
```
ğŸ” è°ƒè¯•: train_dl_modelå¼€å§‹æ‰§è¡Œ
ğŸ” è°ƒè¯•: config['model'] = Transformer
ğŸ“Š è¿‡æ»¤åæ•°æ®ï¼ˆä»2022-01-01å¼€å§‹ï¼‰: 12345è¡Œ
ğŸ“Š è¿‡æ»¤åæ•°æ®ï¼ˆåˆ°2024-09-28ç»“æŸï¼‰: 12345è¡Œ
ğŸ” è°ƒè¯•: å‡†å¤‡å¼€å§‹è®­ç»ƒï¼Œæ¨¡å‹ç±»å‹: DL
ğŸ” è°ƒè¯•: cfg['model'] = Transformer
ğŸ” è°ƒè¯•: cfg['train_params'] = {'batch_size': 64, 'learning_rate': 0.0005, ...}
```

#### 3. è®­ç»ƒè¿›åº¦
```
Epoch 1/50: train_loss=0.1234, val_loss=0.1456, epoch_time=12.34s
Epoch 2/50: train_loss=0.1123, val_loss=0.1345, epoch_time=11.23s
...
æ—©åœäºç¬¬ 25 è½®ï¼ŒéªŒè¯æŸå¤±: 0.0987
```

#### 4. å®éªŒç»“æœ
```
[INFO] Project 1140 | Transformer done, mse=0.0987, rmse=0.3143, mae=0.2456, r_square=0.8765
[METRICS] inference_time=0.1234, param_count=123456, samples_count=1234
[METRICS] best_epoch=25, final_lr=0.000250
[METRICS] nrmse=0.1234, smape=0.0987, gpu_memory_used=1024
ğŸ” è°ƒè¯•: å‡†å¤‡è°ƒç”¨save_resultsï¼Œplant_id=1140
ğŸ” è°ƒè¯•: ä½¿ç”¨é»˜è®¤ä¿å­˜æ¨¡å¼
ğŸ” è°ƒè¯•: save_resultsè°ƒç”¨å®Œæˆ
[INFO] Results saved in ./results/ablation
```

#### 5. å®éªŒå®Œæˆä¿¡æ¯
```
âœ… å®éªŒ åŸå§‹Transformer (Last Timestep) æˆåŠŸå®Œæˆ
â±ï¸  è€—æ—¶: 456.78 ç§’
ğŸ“Š è¾“å‡º:
[INFO] Results saved in ./results/ablation
```

#### 6. æ‰€æœ‰å®éªŒå®Œæˆ
```
ğŸ‰ æ‰€æœ‰å®éªŒå®Œæˆ!
â±ï¸  æ€»è€—æ—¶: 2345.67 ç§’ (39.09 åˆ†é’Ÿ)
ğŸ“Š ç»“æœä¿å­˜åœ¨å„è‡ªçš„resultsç›®å½•ä¸­
```

### ğŸ“ æ–‡ä»¶è¾“å‡ºç»“æ„

å®éªŒå®Œæˆåï¼Œä¼šåœ¨ä»¥ä¸‹ç›®å½•ä¸­ç”Ÿæˆç»“æœæ–‡ä»¶ï¼š

```
Transformer-enhanced/
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ ablation/                    # åŸå§‹Transformerç»“æœ
â”‚   â”‚   â”œâ”€â”€ predictions.csv         # é¢„æµ‹ç»“æœ
â”‚   â”‚   â”œâ”€â”€ training_log.csv        # è®­ç»ƒæ—¥å¿—
â”‚   â”‚   â””â”€â”€ results.xlsx            # Excelç»“æœæ±‡æ€»
â”‚   â”‚
â”‚   â”œâ”€â”€ attention_pooling/           # æ³¨æ„åŠ›æ± åŒ–ç»“æœ
â”‚   â”‚   â”œâ”€â”€ predictions.csv
â”‚   â”‚   â”œâ”€â”€ training_log.csv
â”‚   â”‚   â””â”€â”€ results.xlsx
â”‚   â”‚
â”‚   â”œâ”€â”€ improved_ablation/           # ç»¼åˆæ”¹è¿›ç»“æœ
â”‚   â”‚   â”œâ”€â”€ predictions.csv
â”‚   â”‚   â”œâ”€â”€ training_log.csv
â”‚   â”‚   â””â”€â”€ results.xlsx
â”‚   â”‚
â”‚   â””â”€â”€ hybrid_transformer/          # æ··åˆæ¶æ„ç»“æœ
â”‚       â”œâ”€â”€ predictions.csv
â”‚       â”œâ”€â”€ training_log.csv
â”‚       â””â”€â”€ results.xlsx
```

### ğŸ“Š ç»“æœæ–‡ä»¶è¯¦ç»†è¯´æ˜

#### 1. predictions.csv
åŒ…å«æ¯ä¸ªé¢„æµ‹æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼š
```csv
window_index,forecast_datetime,hour,y_true,y_pred
0,2024-01-01 00:00:00,0,45.2,43.8
0,2024-01-01 01:00:00,1,52.1,50.3
0,2024-01-01 02:00:00,2,48.7,47.2
...
```

**å­—æ®µè¯´æ˜**ï¼š
- `window_index`: é¢„æµ‹çª—å£ç´¢å¼•
- `forecast_datetime`: é¢„æµ‹æ—¶é—´
- `hour`: å°æ—¶ (0-23)
- `y_true`: çœŸå®å‘ç”µé‡ (Capacity Factor %)
- `y_pred`: é¢„æµ‹å‘ç”µé‡ (Capacity Factor %)

#### 2. training_log.csv
åŒ…å«è®­ç»ƒè¿‡ç¨‹çš„è¯¦ç»†ä¿¡æ¯ï¼š
```csv
epoch,train_loss,val_loss,epoch_time,cum_time
1,0.1234,0.1456,12.34,12.34
2,0.1123,0.1345,11.23,23.57
3,0.1045,0.1289,10.98,34.55
...
```

**å­—æ®µè¯´æ˜**ï¼š
- `epoch`: è®­ç»ƒè½®æ¬¡
- `train_loss`: è®­ç»ƒæŸå¤±
- `val_loss`: éªŒè¯æŸå¤±
- `epoch_time`: å•è½®è®­ç»ƒæ—¶é—´(ç§’)
- `cum_time`: ç´¯è®¡è®­ç»ƒæ—¶é—´(ç§’)

#### 3. results.xlsx
Excelæ ¼å¼çš„ç»“æœæ±‡æ€»ï¼ŒåŒ…å«ï¼š

**é…ç½®ä¿¡æ¯**ï¼š
- æ¨¡å‹ç±»å‹ã€å¤æ‚åº¦ã€ç‰¹å¾è®¾ç½®
- è®­ç»ƒå‚æ•°ã€æ•°æ®è®¾ç½®

**æ€§èƒ½æŒ‡æ ‡**ï¼š
- RMSE, MAE, RÂ², NRMSE, SMAPE
- è®­ç»ƒæ—¶é—´ã€æ¨ç†æ—¶é—´
- æ¨¡å‹å‚æ•°æ•°é‡ã€æ ·æœ¬æ•°é‡
- æœ€ä½³epochã€æœ€ç»ˆå­¦ä¹ ç‡
- GPUå†…å­˜ä½¿ç”¨é‡

### ğŸ“ˆ æ€§èƒ½å¯¹æ¯”åˆ†æ

#### å…³é”®æŒ‡æ ‡å¯¹æ¯”
| æ¨¡å‹ | RMSE | MAE | RÂ² | è®­ç»ƒæ—¶é—´ | æ¨ç†æ—¶é—´ | å‚æ•°æ•°é‡ |
|------|------|-----|----|---------|---------|---------| 
| åŸå§‹Transformer | 0.3143 | 0.2456 | 0.8765 | 456s | 0.123s | 123K |
| æ³¨æ„åŠ›æ± åŒ– | 0.2987 | 0.2345 | 0.8843 | 478s | 0.134s | 125K |
| ç»¼åˆæ”¹è¿› | 0.2876 | 0.2234 | 0.8912 | 512s | 0.145s | 128K |
| æ··åˆæ¶æ„ | 0.2934 | 0.2287 | 0.8876 | 567s | 0.156s | 135K |

#### æ”¹è¿›æ•ˆæœåˆ†æ
- **é¢„æµ‹ç²¾åº¦æå‡**: RMSEé™ä½5-15%
- **è®­ç»ƒç¨³å®šæ€§**: å‡å°‘å‘¨æœŸæ€§æ³¢åŠ¨
- **æ”¶æ•›é€Ÿåº¦**: æ—©åœæœºåˆ¶èŠ‚çœè®­ç»ƒæ—¶é—´
- **æ³›åŒ–èƒ½åŠ›**: éªŒè¯é›†æ€§èƒ½æå‡

### ğŸ” ç»“æœåˆ†æå»ºè®®

#### 1. æ€§èƒ½å¯¹æ¯”
```python
# è¯»å–ç»“æœè¿›è¡Œå¯¹æ¯”åˆ†æ
import pandas as pd

# è¯»å–å„æ¨¡å‹çš„ç»“æœ
results = {}
for model in ['ablation', 'attention_pooling', 'improved_ablation', 'hybrid_transformer']:
    results[model] = pd.read_excel(f'results/{model}/results.xlsx')

# å¯¹æ¯”å…³é”®æŒ‡æ ‡
comparison = pd.DataFrame({
    'RMSE': [results[m]['rmse'].iloc[0] for m in results.keys()],
    'MAE': [results[m]['mae'].iloc[0] for m in results.keys()],
    'RÂ²': [results[m]['r_square'].iloc[0] for m in results.keys()]
}, index=results.keys())
```

#### 2. è®­ç»ƒæ›²çº¿åˆ†æ
```python
# åˆ†æè®­ç»ƒè¿‡ç¨‹
import matplotlib.pyplot as plt

for model in results.keys():
    log = pd.read_csv(f'results/{model}/training_log.csv')
    plt.plot(log['epoch'], log['val_loss'], label=model)

plt.xlabel('Epoch')
plt.ylabel('Validation Loss')
plt.legend()
plt.title('Training Curves Comparison')
plt.show()
```

#### 3. é¢„æµ‹ç»“æœåˆ†æ
```python
# åˆ†æé¢„æµ‹å‡†ç¡®æ€§
for model in results.keys():
    pred = pd.read_csv(f'results/{model}/predictions.csv')
    # è®¡ç®—æ¯å°æ—¶çš„å¹³å‡è¯¯å·®
    hourly_error = pred.groupby('hour').apply(
        lambda x: abs(x['y_true'] - x['y_pred']).mean()
    )
    print(f"{model}: å¹³å‡å°æ—¶è¯¯å·® = {hourly_error.mean():.3f}")
```

### âš ï¸ æ³¨æ„äº‹é¡¹

1. **è¿è¡Œæ—¶é—´**: å®Œæ•´å®éªŒå¯èƒ½éœ€è¦30-60åˆ†é’Ÿ
2. **å­˜å‚¨ç©ºé—´**: ç»“æœæ–‡ä»¶çº¦å ç”¨50-100MB
3. **GPUå†…å­˜**: å»ºè®®è‡³å°‘4GB GPUå†…å­˜
4. **æ•°æ®ä¾èµ–**: ç¡®ä¿Project1140.csvæ–‡ä»¶å­˜åœ¨

### ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# è¿è¡Œæ‰€æœ‰å®éªŒ
python run_experiments.py

# è¿è¡Œå•ä¸ªå®éªŒ
python run_experiments.py Transformer_attention_pooling.yaml

# æŸ¥çœ‹ç»“æœ
ls -la results/*/
```

### ğŸ“ é—®é¢˜æ’æŸ¥

å¦‚æœå®éªŒå¤±è´¥ï¼Œæ£€æŸ¥ï¼š
1. æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
2. GPUå†…å­˜æ˜¯å¦è¶³å¤Ÿ
3. ä¾èµ–åŒ…æ˜¯å¦å®‰è£…å®Œæ•´
4. é…ç½®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®
