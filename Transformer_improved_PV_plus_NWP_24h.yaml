data_path: ./Project1140.csv
save_dir: ./results/improved_ablation
future_hours: 24
train_ratio: 0.8
val_ratio: 0.1
plot_days: 7
start_date: '2022-01-01'
end_date: '2024-09-28'

# 训练参数 - 改进版本
train_params:
  batch_size: 64  # 增加批次大小以提高稳定性
  learning_rate: 0.0005
  loss_type: mse
  future_hours: 24

# 保存选项
save_options:
  save_model: false
  save_predictions: true
  save_training_log: true  # 启用训练日志以对比性能
  save_excel_results: true

# 数据配置
past_hours: 24
use_time_encoding: false
weather_category: ablation_11_features
use_pv: true
use_hist_weather: false
use_forecast: true
input_category: PV_plus_NWP

# 模型配置
model: ImprovedTransformer  # 使用改进的Transformer
model_complexity: low

# 改进的模型参数
model_params:
  low:
    d_model: 64
    num_heads: 4
    num_layers: 6
    hidden_dim: 64  # 增加隐藏层维度
    dropout: 0.1
    pooling_type: attention  # 使用注意力池化
    tcn_channels:
    - 64
    - 64
    - 32
    kernel_size: 3
  high:
    d_model: 256
    num_heads: 16
    num_layers: 18
    hidden_dim: 128
    dropout: 0.3
    pooling_type: learned_attention  # 高复杂度使用学习注意力
    tcn_channels:
    - 128
    - 128
    - 64
    - 32
    kernel_size: 3

# 训练轮数配置
epoch_params:
  low:
    epochs: 50  # 增加训练轮数
    tree_params:
      n_estimators: 50
      max_depth: 5
      learning_rate: 0.1
  high:
    epochs: 80  # 增加训练轮数
    tree_params:
      n_estimators: 200
      max_depth: 12
      learning_rate: 0.01

# 实验配置
experiment_name: "Improved_Transformer_PV_NWP"
description: "改进的Transformer架构，使用注意力池化和增强的训练策略"
