# Transformer-Enhanced å¤ªé˜³èƒ½å‘ç”µé¢„æµ‹ç³»ç»Ÿ

## é¡¹ç›®æ¦‚è¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäºTransformerçš„å¤ªé˜³èƒ½å‘ç”µé¢„æµ‹ç³»ç»Ÿï¼Œæ”¯æŒå¤šç§æ”¹è¿›çš„æ¶æ„å’Œè®­ç»ƒç­–ç•¥ã€‚é¡¹ç›®å®ç°äº†åŸå§‹Transformerå’Œæ”¹è¿›ç‰ˆæœ¬çš„å¯¹æ¯”å®éªŒï¼Œç”¨äºè¯„ä¼°ä¸åŒæ¶æ„åœ¨å¤ªé˜³èƒ½é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

## ä¸»è¦ç‰¹æ€§

### ğŸ—ï¸ æ¨¡å‹æ¶æ„
- **åŸå§‹Transformer**: ä½¿ç”¨æœ€åæ—¶é—´æ­¥æ± åŒ–çš„åŸºå‡†æ¨¡å‹
- **æ”¹è¿›Transformer**: æ”¯æŒå¤šç§æ± åŒ–ç­–ç•¥çš„å¢å¼ºç‰ˆæœ¬
- **æ··åˆTransformer**: ç»“åˆEncoder-Decoderå’ŒEncoder-Onlyä¼˜åŠ¿çš„æ··åˆæ¶æ„

### ğŸ¯ æ± åŒ–ç­–ç•¥
- **Last Timestep**: åŸå§‹æ–¹æ³•ï¼Œä½¿ç”¨æœ€åæ—¶é—´æ­¥
- **Mean Pooling**: å¹³å‡æ± åŒ–
- **Max Pooling**: æœ€å¤§æ± åŒ–
- **Attention Pooling**: æ³¨æ„åŠ›æ± åŒ–
- **Learned Attention**: å­¦ä¹ åˆ°çš„æ³¨æ„åŠ›æƒé‡

### ğŸš€ è®­ç»ƒä¼˜åŒ–
- **AdamWä¼˜åŒ–å™¨**: æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›
- **å­¦ä¹ ç‡è°ƒåº¦**: ReduceLROnPlateauè‡ªé€‚åº”è°ƒæ•´
- **æ—©åœæœºåˆ¶**: é˜²æ­¢è¿‡æ‹Ÿåˆ
- **æ¢¯åº¦è£å‰ª**: ç¨³å®šè®­ç»ƒè¿‡ç¨‹
- **åŠ¨æ€ä½ç½®ç¼–ç **: æ”¯æŒæ›´é•¿åºåˆ—

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚
```bash
pip install torch numpy pandas scikit-learn pyyaml
```

### è¿è¡Œæµ‹è¯•
```bash
python test_improvements.py
```

### è¿è¡Œå•ä¸ªå®éªŒ
```bash
# åŸå§‹Transformer
python main.py --config Transformer_low_PV_plus_NWP_24h_noTE.yaml

# æ”¹è¿›Transformer (æ³¨æ„åŠ›æ± åŒ–)
python main.py --config Transformer_attention_pooling.yaml

# æ”¹è¿›Transformer (ç»¼åˆæ”¹è¿›)
python main.py --config Transformer_improved_PV_plus_NWP_24h.yaml

# æ··åˆTransformer
python main.py --config Transformer_hybrid.yaml
```

### è¿è¡Œæ‰€æœ‰å¯¹æ¯”å®éªŒ
```bash
python run_experiments.py
```

## é…ç½®æ–‡ä»¶è¯´æ˜

### 1. Transformer_low_PV_plus_NWP_24h_noTE.yaml
- **ç”¨é€”**: åŸå§‹TransformeråŸºå‡†æ¨¡å‹
- **ç‰¹ç‚¹**: ä½¿ç”¨æœ€åæ—¶é—´æ­¥æ± åŒ–
- **æ•°æ®**: PV + NWPé¢„æµ‹æ•°æ®

### 2. Transformer_attention_pooling.yaml
- **ç”¨é€”**: æ³¨æ„åŠ›æ± åŒ–æ”¹è¿›
- **ç‰¹ç‚¹**: ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶èšåˆåºåˆ—ä¿¡æ¯
- **ä¼˜åŠ¿**: æ›´å¥½åœ°åˆ©ç”¨å…¨å±€ä¿¡æ¯

### 3. Transformer_improved_PV_plus_NWP_24h.yaml
- **ç”¨é€”**: ç»¼åˆæ”¹è¿›ç‰ˆæœ¬
- **ç‰¹ç‚¹**: åŒ…å«æ‰€æœ‰æ”¹è¿›ç­–ç•¥
- **ä¼˜åŠ¿**: æœ€ä½³æ€§èƒ½é¢„æœŸ

### 4. Transformer_hybrid.yaml
- **ç”¨é€”**: æ··åˆæ¶æ„
- **ç‰¹ç‚¹**: ç»“åˆEncoder-Decoderä¼˜åŠ¿
- **ä¼˜åŠ¿**: æ›´å¤æ‚çš„å»ºæ¨¡èƒ½åŠ›

## å®éªŒå¯¹æ¯”

### æ€§èƒ½æŒ‡æ ‡
- **RMSE**: å‡æ–¹æ ¹è¯¯å·®
- **MAE**: å¹³å‡ç»å¯¹è¯¯å·®
- **RÂ²**: å†³å®šç³»æ•°
- **è®­ç»ƒæ—¶é—´**: æ”¶æ•›é€Ÿåº¦
- **æ¨ç†æ—¶é—´**: é¢„æµ‹æ•ˆç‡

### é¢„æœŸæ”¹è¿›
- **é¢„æµ‹ç²¾åº¦**: RMSE/MAEé™ä½5-15%
- **è®­ç»ƒç¨³å®šæ€§**: å‡å°‘å‘¨æœŸæ€§æ³¢åŠ¨
- **æ”¶æ•›é€Ÿåº¦**: æå‡20-30%
- **æ³›åŒ–èƒ½åŠ›**: æ›´å¥½çš„éªŒè¯é›†æ€§èƒ½

## é¡¹ç›®ç»“æ„

```
Transformer-enhanced/
â”œâ”€â”€ main.py                          # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ transformer.py                   # åŸå§‹Transformerå®ç°
â”œâ”€â”€ transformer_improved.py          # æ”¹è¿›çš„Transformerå®ç°
â”œâ”€â”€ train_dl.py                      # æ·±åº¦å­¦ä¹ è®­ç»ƒä»£ç 
â”œâ”€â”€ data_utils.py                    # æ•°æ®å¤„ç†å·¥å…·
â”œâ”€â”€ eval_utils.py                    # è¯„ä¼°å·¥å…·
â”œâ”€â”€ train_utils.py                   # è®­ç»ƒå·¥å…·
â”œâ”€â”€ gpu_utils.py                     # GPUç›‘æ§å·¥å…·
â”œâ”€â”€ run_experiments.py               # å®éªŒè¿è¡Œè„šæœ¬
â”œâ”€â”€ test_improvements.py             # æµ‹è¯•è„šæœ¬
â”œâ”€â”€ *.yaml                           # é…ç½®æ–‡ä»¶
â”œâ”€â”€ IMPROVEMENTS.md                  # æ”¹è¿›è¯´æ˜
â””â”€â”€ README.md                        # é¡¹ç›®è¯´æ˜
```

## æŠ€æœ¯ç»†èŠ‚

### æ”¹è¿›çš„Transformeræ¶æ„
```python
class ImprovedTransformer(nn.Module):
    def __init__(self, hist_dim, fcst_dim, config):
        # è¾“å…¥æŠ•å½±å±‚
        self.hist_proj = nn.Linear(hist_dim, d_model)
        self.fcst_proj = nn.Linear(fcst_dim, d_model)
        
        # ä½ç½®ç¼–ç 
        self.pos_enc = LocalPositionalEncoding(d_model)
        
        # Transformerç¼–ç å™¨
        self.encoder = nn.TransformerEncoder(...)
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pool = AttentionPooling(d_model)
        
        # æ”¹è¿›çš„è¾“å‡ºå¤´
        self.output_head = nn.Sequential(...)
```

### æ³¨æ„åŠ›æ± åŒ–æœºåˆ¶
```python
class AttentionPooling(nn.Module):
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ›æ± åŒ–
        attended, _ = self.attention(query, x, x)
        return attended.squeeze(1)
```

## ç»“æœåˆ†æ

### å®éªŒè¾“å‡º
æ¯ä¸ªå®éªŒçš„ç»“æœä¿å­˜åœ¨ç‹¬ç«‹çš„ç›®å½•ä¸­ï¼š
- `results/ablation/`: åŸå§‹æ¨¡å‹ç»“æœ
- `results/attention_pooling/`: æ³¨æ„åŠ›æ± åŒ–ç»“æœ
- `results/improved_ablation/`: ç»¼åˆæ”¹è¿›ç»“æœ
- `results/hybrid_transformer/`: æ··åˆæ¶æ„ç»“æœ

### ç»“æœæ–‡ä»¶
- `predictions.csv`: é¢„æµ‹ç»“æœ
- `training_log.csv`: è®­ç»ƒæ—¥å¿—
- `results.xlsx`: Excelæ ¼å¼ç»“æœæ±‡æ€»

## è‡ªå®šä¹‰å®éªŒ

### åˆ›å»ºæ–°é…ç½®
1. å¤åˆ¶ç°æœ‰é…ç½®æ–‡ä»¶
2. ä¿®æ”¹æ¨¡å‹å‚æ•°
3. è°ƒæ•´è®­ç»ƒè®¾ç½®
4. è¿è¡Œå®éªŒ

### æ·»åŠ æ–°æ± åŒ–ç­–ç•¥
1. åœ¨`ImprovedTransformer`ä¸­æ·»åŠ æ–°æ–¹æ³•
2. æ›´æ–°`pooling_type`é€‰é¡¹
3. æµ‹è¯•æ–°ç­–ç•¥

## è´¡çŒ®æŒ‡å—

1. Forké¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. åˆ›å»ºPull Request

## è®¸å¯è¯

MIT License

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·åˆ›å»ºIssueæˆ–è”ç³»é¡¹ç›®ç»´æŠ¤è€…ã€‚

---

**æ³¨æ„**: è¿™æ˜¯ä¸€ä¸ªç ”ç©¶é¡¹ç›®ï¼Œç”¨äºå¯¹æ¯”ä¸åŒTransformeræ¶æ„åœ¨å¤ªé˜³èƒ½é¢„æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚æ‰€æœ‰æ”¹è¿›éƒ½ç»è¿‡æµ‹è¯•éªŒè¯ï¼Œå¯ä»¥å®‰å…¨ä½¿ç”¨ã€‚
